{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3f6ee55-e7f0-4fba-bc43-0a0c240ed7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in ./env/lib/python3.13/site-packages (1.11.0)\n",
      "Collecting futures\n",
      "  Downloading futures-3.0.5.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: langchain-community in ./env/lib/python3.13/site-packages (0.3.24)\n",
      "Requirement already satisfied: python-dotenv in ./env/lib/python3.13/site-packages (1.1.0)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./env/lib/python3.13/site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in ./env/lib/python3.13/site-packages (from faiss-cpu) (24.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.59 in ./env/lib/python3.13/site-packages (from langchain-community) (0.3.60)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.25 in ./env/lib/python3.13/site-packages (from langchain-community) (0.3.25)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./env/lib/python3.13/site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in ./env/lib/python3.13/site-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./env/lib/python3.13/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./env/lib/python3.13/site-packages (from langchain-community) (3.11.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./env/lib/python3.13/site-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./env/lib/python3.13/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./env/lib/python3.13/site-packages (from langchain-community) (2.9.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./env/lib/python3.13/site-packages (from langchain-community) (0.3.42)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./env/lib/python3.13/site-packages (from langchain-community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./env/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./env/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./env/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./env/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.25->langchain-community) (2.11.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.59->langchain-community) (4.13.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./env/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.59->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: anyio in ./env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: idna in ./env/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./env/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.25->langchain-community) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
      "Requirement already satisfied: greenlet>=1 in ./env/lib/python3.13/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./env/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./env/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
      "Building wheels for collected packages: futures\n",
      "\u001b[33m  DEPRECATION: Building 'futures' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'futures'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for futures (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for futures: filename=futures-3.0.5-py3-none-any.whl size=14144 sha256=2a7630bc3143caa0cab8b5faabe6cbfe56481b557779d2270c99490e4ac26673\n",
      "  Stored in directory: /Users/softaims/Library/Caches/pip/wheels/c0/65/47/17d11231c90639e4f76ee5e86b7cbd607b97f11d7677789787\n",
      "Successfully built futures\n",
      "Installing collected packages: futures\n",
      "Successfully installed futures-3.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu futures langchain-community python-dotenv tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49aa0cc0-6db4-4829-a347-a870773e8a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable (comment out if not using OpenAI)\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"Please enter your OpenAI API key: \")\n",
    "else:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Original path append replaced for Colab compatibility\n",
    "from helper_functions import *\n",
    "from evaluation.evalute_rag import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d495d97f-80a9-4e69-bad6-2fc044da35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/Understanding_Climate_Change.pdf\"\n",
    "LANGUAGE_MODEL_NAME = \"gpt-4o-mini\"\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-3-small\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32dadbb4-c7c3-4019-878a-82794a1055b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hypothetical_prompt_embeddings(chunk_text: str):\n",
    "    \"\"\"\n",
    "    Uses the LLM to generate multiple hypothetical questions for a single chunk.\n",
    "    These questions will be used as 'proxies' for the chunk during retrieval.\n",
    "\n",
    "    Parameters:\n",
    "    chunk_text (str): Text contents of the chunk\n",
    "\n",
    "    Returns:\n",
    "    chunk_text (str): Text contents of the chunk. This is done to make the \n",
    "        multithreading easier\n",
    "    hypothetical prompt embeddings (List[float]): A list of embedding vectors\n",
    "        generated from the questions\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(temperature=0, model_name=LANGUAGE_MODEL_NAME)\n",
    "    embedding_model = OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    question_gen_prompt = PromptTemplate.from_template(\n",
    "        \"Analyze the input text and generate essential questions that, when answered, \\\n",
    "        capture the main points of the text. Each question should be one line, \\\n",
    "        without numbering or prefixes.\\n\\n \\\n",
    "        Text:\\n{chunk_text}\\n\\nQuestions:\\n\"\n",
    "    )\n",
    "    question_chain = question_gen_prompt | llm | StrOutputParser()\n",
    "\n",
    "    # parse questions from response\n",
    "    # Notes: \n",
    "    # - gpt4o likes to split questions by \\n\\n so we remove one \\n\n",
    "    # - for production or if using smaller models from ollama, it's beneficial to use regex to parse \n",
    "    # things like (un)ordeed lists\n",
    "    # r\"^\\s*[\\-\\*\\•]|\\s*\\d+\\.\\s*|\\s*[a-zA-Z]\\)\\s*|\\s*\\(\\d+\\)\\s*|\\s*\\([a-zA-Z]\\)\\s*|\\s*\\([ivxlcdm]+\\)\\s*\"\n",
    "    questions = question_chain.invoke({\"chunk_text\": chunk_text}).replace(\"\\n\\n\", \"\\n\").split(\"\\n\")\n",
    "    \n",
    "    return chunk_text, embedding_model.embed_documents(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d220efdf-fefb-4129-a2c4-8af6e9626ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vector_store(chunks: List[str]):\n",
    "    \"\"\"\n",
    "    Creates and populates a FAISS vector store from a list of text chunks.\n",
    "\n",
    "    This function processes a list of text chunks in parallel, generating \n",
    "    hypothetical prompt embeddings for each chunk.\n",
    "    The embeddings are stored in a FAISS index for efficient similarity search.\n",
    "\n",
    "    Parameters:\n",
    "    chunks (List[str]): A list of text chunks to be embedded and stored.\n",
    "\n",
    "    Returns:\n",
    "    FAISS: A FAISS vector store containing the embedded text chunks.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wait with initialization to see vector lengths\n",
    "    vector_store = None  \n",
    "\n",
    "    with ThreadPoolExecutor() as pool:  \n",
    "        # Use threading to speed up generation of prompt embeddings\n",
    "        futures = [pool.submit(generate_hypothetical_prompt_embeddings, c) for c in chunks]\n",
    "        \n",
    "        # Process embeddings as they complete\n",
    "        for f in tqdm(as_completed(futures), total=len(chunks)):  \n",
    "            \n",
    "            chunk, vectors = f.result()  # Retrieve the processed chunk and its embeddings\n",
    "            \n",
    "            # Initialize the FAISS vector store on the first chunk\n",
    "            if vector_store == None:  \n",
    "                vector_store = FAISS(\n",
    "                    embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL_NAME),  # Define embedding model\n",
    "                    index=faiss.IndexFlatL2(len(vectors[0])),  # Define an L2 index for similarity search\n",
    "                    docstore=InMemoryDocstore(),  # Use in-memory document storage\n",
    "                    index_to_docstore_id={}  # Maintain index-to-document mapping\n",
    "                )\n",
    "            \n",
    "            # Pair the chunk's content with each generated embedding vector.\n",
    "            # Each chunk is inserted multiple times, once for each prompt vector\n",
    "            chunks_with_embedding_vectors = [(chunk.page_content, vec) for vec in vectors]\n",
    "            \n",
    "            # Add embeddings to the store\n",
    "            vector_store.add_embeddings(chunks_with_embedding_vectors)  \n",
    "\n",
    "    return vector_store  # Return the populated vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73193b79-1a5a-4f2d-a959-93cd4ffdb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pdf(path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Encodes a PDF book into a vector store using OpenAI embeddings.\n",
    "\n",
    "    Args:\n",
    "        path: The path to the PDF file.\n",
    "        chunk_size: The desired size of each text chunk.\n",
    "        chunk_overlap: The amount of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        A FAISS vector store containing the encoded book content.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "\n",
    "    vectorstore = prepare_vector_store(cleaned_texts)\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8fb0cfb-5f11-4e0d-8180-e9490d344b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 97/97 [00:50<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Chunk size can be quite large with HyPE as we are not loosing percision with more\n",
    "# information. For production, test how exhaustive your model is in generating sufficient \n",
    "# amount of questions per chunk. This will mostly depend on your information density.\n",
    "chunks_vector_store = encode_pdf(PATH, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00c3c271-a34a-40a2-b185-3e7cdd7617a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_query_retriever = chunks_vector_store.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2b7363-f1d8-4562-a3d4-479a0b954a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context 1:\n",
      "Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives. During the Holocene epoch, which \n",
      "began at the end of the last ice age, human societies flourished, but the industrial era has seen \n",
      "unprecedented changes. \n",
      "Modern Observations \n",
      "Modern scientific observations indicate a rapid increase in global temperatures, sea levels, \n",
      "and extreme weather events. The Intergovernmental Panel on Climate Change (IPCC) has \n",
      "documented these changes extensively. Ice core samples, tree rings, and ocean sediments \n",
      "provide a historical record that scientists use to understand past climate conditions and \n",
      "predict future trends. The evidence overwhelmingly shows that recent changes are primarily \n",
      "driven by human activities, particularly the emission of greenhouse gases. \n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the\n",
      "\n",
      "\n",
      "Context 2:\n",
      "Chapter 2: Causes of Climate Change \n",
      "Greenhouse Gases \n",
      "The primary cause of recent climate change is the increase in greenhouse gases in the \n",
      "atmosphere. Greenhouse gases, such as carbon dioxide (CO2), methane (CH4), and nitrous \n",
      "oxide (N2O), trap heat from the sun, creating a \"greenhouse effect.\" This effect is essential \n",
      "for life on Earth, as it keeps the planet warm enough to support life. However, human \n",
      "activities have intensified this natural process, leading to a warmer climate. \n",
      "Fossil Fuels \n",
      "Burning fossil fuels for energy releases large amounts of CO2. This includes coal, oil, and \n",
      "natural gas used for electricity, heating, and transportation. The industrial revolution marked \n",
      "the beginning of a significant increase in fossil fuel consumption, which continues to rise \n",
      "today. \n",
      "Coal\n",
      "\n",
      "\n",
      "Context 3:\n",
      "Understanding Climate Change \n",
      "Chapter 1: Introduction to Climate Change \n",
      "Climate change refers to significant, long-term changes in the global climate. The term \n",
      "\"global climate\" encompasses the planet's overall weather patterns, including temperature, \n",
      "precipitation, and wind patterns, over an extended period. Over the past century, human \n",
      "activities, particularly the burning of fossil fuels and deforestation, have significantly \n",
      "contributed to climate change. \n",
      "Historical Context \n",
      "The Earth's climate has changed throughout history. Over the past 650,000 years, there have \n",
      "been seven cycles of glacial advance and retreat, with the abrupt end of the last ice age about \n",
      "11,700 years ago marking the beginning of the modern climate era and human civilization. \n",
      "Most of these climate changes are attributed to very small variations in Earth's orbit that \n",
      "change the amount of solar energy our planet receives. During the Holocene epoch, which\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_query = \"What is the main cause of climate change?\"\n",
    "context = retrieve_context_per_question(test_query, chunks_query_retriever)\n",
    "context = list(set(context))\n",
    "show_context(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c0402-9cc7-476d-9ec3-5a2477386d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
