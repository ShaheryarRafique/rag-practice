{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c36d177-40df-4316-b622-bff2671708df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./env/lib/python3.13/site-packages (from -r requirement.txt (line 1)) (0.3.25)\n",
      "Requirement already satisfied: langchain-community in ./env/lib/python3.13/site-packages (from -r requirement.txt (line 2)) (0.3.23)\n",
      "Requirement already satisfied: python-dotenv in ./env/lib/python3.13/site-packages (from -r requirement.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: faiss-cpu in ./env/lib/python3.13/site-packages (from -r requirement.txt (line 4)) (1.11.0)\n",
      "Requirement already satisfied: pandas in ./env/lib/python3.13/site-packages (from -r requirement.txt (line 5)) (2.2.3)\n",
      "Requirement already satisfied: langchain-openai in ./env/lib/python3.13/site-packages (from -r requirement.txt (line 6)) (0.3.16)\n",
      "Collecting chromadb (from -r requirement.txt (line 7))\n",
      "  Downloading chromadb-1.0.8-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in ./env/lib/python3.13/site-packages (from langchain->-r requirement.txt (line 1)) (0.3.59)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in ./env/lib/python3.13/site-packages (from langchain->-r requirement.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in ./env/lib/python3.13/site-packages (from langchain->-r requirement.txt (line 1)) (0.3.42)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./env/lib/python3.13/site-packages (from langchain->-r requirement.txt (line 1)) (2.11.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./env/lib/python3.13/site-packages (from langchain->-r requirement.txt (line 1)) (2.0.40)\n",
      "Requirement already satisfied: requests<3,>=2 in ./env/lib/python3.13/site-packages (from langchain->-r requirement.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./env/lib/python3.13/site-packages (from langchain->-r requirement.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./env/lib/python3.13/site-packages (from langchain-community->-r requirement.txt (line 2)) (3.11.18)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./env/lib/python3.13/site-packages (from langchain-community->-r requirement.txt (line 2)) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./env/lib/python3.13/site-packages (from langchain-community->-r requirement.txt (line 2)) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in ./env/lib/python3.13/site-packages (from langchain-community->-r requirement.txt (line 2)) (2.9.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./env/lib/python3.13/site-packages (from langchain-community->-r requirement.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: numpy>=2.1.0 in ./env/lib/python3.13/site-packages (from langchain-community->-r requirement.txt (line 2)) (2.2.5)\n",
      "Requirement already satisfied: packaging in ./env/lib/python3.13/site-packages (from faiss-cpu->-r requirement.txt (line 4)) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.13/site-packages (from pandas->-r requirement.txt (line 5)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.13/site-packages (from pandas->-r requirement.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.13/site-packages (from pandas->-r requirement.txt (line 5)) (2025.2)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.68.2 in ./env/lib/python3.13/site-packages (from langchain-openai->-r requirement.txt (line 6)) (1.78.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in ./env/lib/python3.13/site-packages (from langchain-openai->-r requirement.txt (line 6)) (0.9.0)\n",
      "Collecting build>=1.0.3 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting fastapi==0.115.9 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./env/lib/python3.13/site-packages (from chromadb->-r requirement.txt (line 7)) (4.13.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading onnxruntime-1.22.0-cp313-cp313-macosx_13_0_universal2.whl.metadata (4.5 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading opentelemetry_api-1.33.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.54b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading opentelemetry_sdk-1.33.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb->-r requirement.txt (line 7))\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in ./env/lib/python3.13/site-packages (from chromadb->-r requirement.txt (line 7)) (4.67.1)\n",
      "Collecting overrides>=7.3.1 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl.metadata (3.8 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading typer-0.15.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading mmh3-5.1.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./env/lib/python3.13/site-packages (from chromadb->-r requirement.txt (line 7)) (3.10.18)\n",
      "Requirement already satisfied: httpx>=0.27.0 in ./env/lib/python3.13/site-packages (from chromadb->-r requirement.txt (line 7)) (0.28.1)\n",
      "Collecting rich>=10.11.0 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting jsonschema>=4.19.0 (from chromadb->-r requirement.txt (line 7))\n",
      "  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirement.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirement.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirement.txt (line 2)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirement.txt (line 2)) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirement.txt (line 2)) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirement.txt (line 2)) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./env/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community->-r requirement.txt (line 2)) (1.20.0)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./env/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirement.txt (line 2)) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./env/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirement.txt (line 2)) (0.9.0)\n",
      "Requirement already satisfied: anyio in ./env/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb->-r requirement.txt (line 7)) (4.9.0)\n",
      "Requirement already satisfied: certifi in ./env/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb->-r requirement.txt (line 7)) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in ./env/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb->-r requirement.txt (line 7)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./env/lib/python3.13/site-packages (from httpx>=0.27.0->chromadb->-r requirement.txt (line 7)) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in ./env/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb->-r requirement.txt (line 7)) (0.16.0)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=4.19.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.19.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=4.19.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading rpds_py-0.24.0-cp313-cp313-macosx_10_12_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in ./env/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7)) (1.17.0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./env/lib/python3.13/site-packages (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7)) (2.4.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./env/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.58->langchain->-r requirement.txt (line 1)) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirement.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./env/lib/python3.13/site-packages (from langsmith<0.4,>=0.1.17->langchain->-r requirement.txt (line 1)) (0.23.0)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting sympy (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./env/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirement.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in ./env/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirement.txt (line 6)) (0.9.0)\n",
      "Requirement already satisfied: sniffio in ./env/lib/python3.13/site-packages (from openai<2.0.0,>=1.68.2->langchain-openai->-r requirement.txt (line 6)) (1.3.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib-metadata<8.7.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.33.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.33.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.33.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading opentelemetry_proto-1.33.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.54b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.54b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.54b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading opentelemetry_instrumentation-0.54b0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.54b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading opentelemetry_util_http-0.54b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached wrapt-1.17.2-cp313-cp313-macosx_10_13_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.54b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 1)) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./env/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain->-r requirement.txt (line 1)) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.13/site-packages (from requests<3,>=2->langchain->-r requirement.txt (line 1)) (3.4.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: greenlet>=1 in ./env/lib/python3.13/site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirement.txt (line 1)) (3.2.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./env/lib/python3.13/site-packages (from tiktoken<1,>=0.7->langchain-openai->-r requirement.txt (line 6)) (2024.11.6)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from tokenizers>=0.13.2->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click>=8.0.0 (from typer>=0.9.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading httptools-0.6.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (3.6 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading uvloop-0.21.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading watchfiles-1.0.5-cp313-cp313-macosx_10_12_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading websockets-15.0.1-cp313-cp313-macosx_10_13_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting filelock (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached hf_xet-1.1.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (494 bytes)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./env/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain->-r requirement.txt (line 1)) (3.0.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./env/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->-r requirement.txt (line 2)) (1.1.0)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime>=1.14.1->chromadb->-r requirement.txt (line 7))\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb->-r requirement.txt (line 7))\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading chromadb-1.0.8-cp39-abi3-macosx_10_12_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:04\u001b[0mm\n",
      "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-macosx_10_12_universal2.whl (498 kB)\n",
      "Downloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading grpcio-1.71.0-cp313-cp313-macosx_10_14_universal2.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m02\u001b[0m\n",
      "\u001b[?25hDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading mmh3-5.1.0-cp313-cp313-macosx_10_13_x86_64.whl (40 kB)\n",
      "Downloading onnxruntime-1.22.0-cp313-cp313-macosx_13_0_universal2.whl (34.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.3/34.3 MB\u001b[0m \u001b[31m944.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:02\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading opentelemetry_api-1.33.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.33.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.33.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.33.0-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.54b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.54b0-py3-none-any.whl (31 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.54b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl (194 kB)\n",
      "Downloading opentelemetry_util_http-0.54b0-py3-none-any.whl (7.3 kB)\n",
      "Downloading opentelemetry_sdk-1.33.0-py3-none-any.whl (118 kB)\n",
      "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
      "Downloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "Downloading typer-0.15.3-py3-none-any.whl (45 kB)\n",
      "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httptools-0.6.4-cp313-cp313-macosx_10_13_universal2.whl (197 kB)\n",
      "Using cached huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-5.29.4-cp38-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Downloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m456.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading rpds_py-0.24.0-cp313-cp313-macosx_10_12_x86_64.whl (367 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
      "Downloading uvloop-0.21.0-cp313-cp313-macosx_10_13_x86_64.whl (819 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m539.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-1.0.5-cp313-cp313-macosx_10_12_x86_64.whl (401 kB)\n",
      "Downloading websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-macosx_10_13_x86_64.whl (173 kB)\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m0m\n",
      "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached hf_xet-1.1.0-cp37-abi3-macosx_10_12_x86_64.whl (5.1 MB)\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached wrapt-1.17.2-cp313-cp313-macosx_10_13_x86_64.whl (38 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: pypika\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=309b32ce57b0f50be8d719fa0b0b0804c80b5ad70cfcea90430dbdee82a543e2\n",
      "  Stored in directory: /Users/softaims/Library/Caches/pip/wheels/b4/f8/a5/28e9c1524d320f4b8eefdce0e487b5c2e128dbf2ed1bb4a60b\n",
      "Successfully built pypika\n",
      "Installing collected packages: pypika, mpmath, flatbuffers, durationpy, zipp, wrapt, websockets, websocket-client, uvloop, sympy, shellingham, rpds-py, pyproject_hooks, pygments, pyasn1, protobuf, overrides, opentelemetry-util-http, oauthlib, mmh3, mdurl, importlib-resources, humanfriendly, httptools, hf-xet, grpcio, fsspec, filelock, click, cachetools, bcrypt, backoff, asgiref, watchfiles, uvicorn, starlette, rsa, requests-oauthlib, referencing, pyasn1-modules, posthog, opentelemetry-proto, markdown-it-py, importlib-metadata, huggingface-hub, googleapis-common-protos, deprecated, coloredlogs, build, tokenizers, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, jsonschema-specifications, google-auth, fastapi, typer, opentelemetry-semantic-conventions, kubernetes, jsonschema, opentelemetry-sdk, opentelemetry-instrumentation, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 cachetools-5.5.2 chromadb-1.0.8 click-8.2.0 coloredlogs-15.0.1 deprecated-1.2.18 durationpy-0.9 fastapi-0.115.9 filelock-3.18.0 flatbuffers-25.2.10 fsspec-2025.3.2 google-auth-2.40.1 googleapis-common-protos-1.70.0 grpcio-1.71.0 hf-xet-1.1.0 httptools-0.6.4 huggingface-hub-0.31.1 humanfriendly-10.0 importlib-metadata-8.6.1 importlib-resources-6.5.2 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 kubernetes-32.0.1 markdown-it-py-3.0.0 mdurl-0.1.2 mmh3-5.1.0 mpmath-1.3.0 oauthlib-3.2.2 onnxruntime-1.22.0 opentelemetry-api-1.33.0 opentelemetry-exporter-otlp-proto-common-1.33.0 opentelemetry-exporter-otlp-proto-grpc-1.33.0 opentelemetry-instrumentation-0.54b0 opentelemetry-instrumentation-asgi-0.54b0 opentelemetry-instrumentation-fastapi-0.54b0 opentelemetry-proto-1.33.0 opentelemetry-sdk-1.33.0 opentelemetry-semantic-conventions-0.54b0 opentelemetry-util-http-0.54b0 overrides-7.7.0 posthog-4.0.1 protobuf-5.29.4 pyasn1-0.6.1 pyasn1-modules-0.4.2 pygments-2.19.1 pypika-0.48.9 pyproject_hooks-1.2.0 referencing-0.36.2 requests-oauthlib-2.0.0 rich-14.0.0 rpds-py-0.24.0 rsa-4.9.1 shellingham-1.5.4 starlette-0.45.3 sympy-1.14.0 tokenizers-0.21.1 typer-0.15.3 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5 websocket-client-1.8.0 websockets-15.0.1 wrapt-1.17.2 zipp-3.21.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad505f81-a34f-4f7c-b7e6-f4b16929c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLMs\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from '.env' file\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd241310-1f48-4308-b682-5a8f9b6110db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build Index\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI,OpenAIEmbeddings\n",
    "\n",
    "# Set embeddings\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-3-tool-use/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/?ref=dl-staging-website.ghost.io\",\n",
    "    \"https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\"\n",
    "]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag\",\n",
    "    embedding=embedding_model,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 4}, # number of documents to retrieve\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad228b52-a9d6-403e-9cb8-ac059fa8f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the differnt kind of agentic design patterns?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6684fc8c-c302-4e9a-a0ff-5318134b7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a927bcf-bbd3-4cc7-8649-9445d41803d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Agentic Design Patterns Part 5, Multi-Agent Collaboration\n",
      "\n",
      "Source: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\n",
      "\n",
      "Content: scalable and highly secure code. By decomposing the overall task into subtasks, we can optimize the subtasks better.Perhaps most important, the multi-agent design pattern gives us, as developers, a framework for breaking down complex tasks into subtasks. When writing code to run on a single CPU, we often break our program up into different processes or threads. This is a useful abstraction that lets us decompose a task, like implementing a web browser, into subtasks that are easier to code. I find thinking through multi-agent roles to be a useful abstraction as well.In many companies, managers routinely decide what roles to hire, and then how to split complex projects — like writing a large piece of software or preparing a research report — into smaller tasks to assign to employees with different specialties. Using multiple agents is analogous. Each agent implements its own workflow, has its own memory (itself a rapidly evolving area in agentic technology: how can an agent remember enough of its past interactions to perform better on upcoming ones?), and may ask other agents for help. Agents can also engage in Planning and Tool Use. This results in a cacophony of LLM calls and message passing between agents that can result in very complex workflows. While managing people is hard, it's a sufficiently familiar idea that it gives us a mental framework for how to \"hire\" and assign tasks to our AI agents. Fortunately, the damage from mismanaging an AI agent is much lower than that from mismanaging humans! Emerging frameworks like AutoGen, Crew AI, and LangGraph, provide rich ways to build multi-agent solutions to problems. If you're interested in playing with a fun multi-agent system, also check out ChatDev, an open source implementation of a set of agents that run a virtual software company. I encourage you to check out their GitHub repo and perhaps clone the repo and run the system yourself. While it may not always produce what you want, you might be amazed at how well it does. Like the design pattern of Planning, I find the output quality of multi-agent collaboration hard to predict, especially when allowing agents to interact freely and providing them with multiple tools. The more mature patterns of Reflection and Tool Use are more reliable. I hope you enjoy playing with these agentic design patterns and that they produce amazing results for you! If you're interested in learning more, I\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Title: {docs[0].metadata['title']}\\n\\nSource: {docs[0].metadata['source']}\\n\\nContent: {docs[0].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d776caba-c9df-4f88-be5e-3b7de38f001c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Check document relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86c84520-99e2-477b-b6cf-395d959edb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "635b90f5-0fd8-46fd-8723-9bde65c2d474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalable and highly secure code. By decomposing the overall task into subtasks, we can optimize the subtasks better.Perhaps most important, the multi-agent design pattern gives us, as developers, a framework for breaking down complex tasks into subtasks. When writing code to run on a single CPU, we often break our program up into different processes or threads. This is a useful abstraction that lets us decompose a task, like implementing a web browser, into subtasks that are easier to code. I find thinking through multi-agent roles to be a useful abstraction as well.In many companies, managers routinely decide what roles to hire, and then how to split complex projects — like writing a large piece of software or preparing a research report — into smaller tasks to assign to employees with different specialties. Using multiple agents is analogous. Each agent implements its own workflow, has its own memory (itself a rapidly evolving area in agentic technology: how can an agent remember enough of its past interactions to perform better on upcoming ones?), and may ask other agents for help. Agents can also engage in Planning and Tool Use. This results in a cacophony of LLM calls and message passing between agents that can result in very complex workflows. While managing people is hard, it's a sufficiently familiar idea that it gives us a mental framework for how to \"hire\" and assign tasks to our AI agents. Fortunately, the damage from mismanaging an AI agent is much lower than that from mismanaging humans! Emerging frameworks like AutoGen, Crew AI, and LangGraph, provide rich ways to build multi-agent solutions to problems. If you're interested in playing with a fun multi-agent system, also check out ChatDev, an open source implementation of a set of agents that run a virtual software company. I encourage you to check out their GitHub repo and perhaps clone the repo and run the system yourself. While it may not always produce what you want, you might be amazed at how well it does. Like the design pattern of Planning, I find the output quality of multi-agent collaboration hard to predict, especially when allowing agents to interact freely and providing them with multiple tools. The more mature patterns of Reflection and Tool Use are more reliable. I hope you enjoy playing with these agentic design patterns and that they produce amazing results for you! If you're interested in learning more, I \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "Agentic Design Patterns Part 4: Planning✨ New course! Enroll in Building AI Voice Agents for ProductionExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 4, Planning Large language models can drive powerful agents to execute complex tasks if you ask them to plan the steps before they act.LettersTechnical InsightsPublishedApr 10, 2024Reading time3 min readShareDear friends,Planning is a key agentic AI design pattern in which we use a large language model (LLM) to autonomously decide on what sequence of steps to execute to accomplish a larger task. For example, if we ask an agent to do online research on a given topic, we might use an LLM to break down the objective into smaller subtasks, such as researching specific subtopics, synthesizing findings, and compiling a report. Many people had a “ChatGPT moment” shortly after ChatGPT was released, when they played with it and were surprised that it significantly exceeded their expectation of what AI can do. If you have not yet had a similar “AI Agentic moment,” I hope you will soon. I had one several months ago, when I presented a live demo of a research agent I had implemented that had access to various online search tools. I had tested this agent multiple times privately, during which it consistently used a web search tool to gather information and wrote up a summary. During the live demo, though, the web search API unexpectedly returned with a rate limiting error. I thought my demo was about to fail publicly, and I dreaded what was to come next. To my surprise, the agent pivoted deftly to a Wikipedia search tool — which I had forgotten I’d given it — and completed the task using Wikipedia instead of web search. This was an AI Agentic moment of surprise for me. I think many people who haven’t experienced such a moment yet will do so in the coming months. It’s a beautiful thing when you see an agent autonomously decide to do things in ways that you had not anticipated, and succeed as a result!Many tasks can’t be done in a single step or with \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "Agentic Design Patterns Part 5, Multi-Agent Collaboration✨ New course! Enroll in Building AI Voice Agents for ProductionExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 5, Multi-Agent Collaboration Prompting an LLM to play different roles for different parts of a complex task summons a team of AI agents that can do the job more effectively.LettersTechnical InsightsPublishedApr 17, 2024Reading time3 min readShareDear friends,Multi-agent collaboration is the last of the four key AI agentic design patterns that I’ve described in recent letters. Given a complex task like writing software, a multi-agent approach would break down the task into subtasks to be executed by different roles — such as a software engineer, product manager, designer, QA (quality assurance) engineer, and so on — and have different agents accomplish different subtasks.Different agents might be built by prompting one LLM (or, if you prefer, multiple LLMs) to carry out different tasks. For example, to build a software engineer agent, we might prompt the LLM: “You are an expert in writing clear, efficient code. Write code to perform the task . . ..” It might seem counterintuitive that, although we are making multiple calls to the same LLM, we apply the programming abstraction of using multiple agents. I’d like to offer a few reasons:It works! Many teams are getting good results with this method, and there’s nothing like results! Further, ablation studies (for example, in the AutoGen paper cited below) show that multiple agents give superior performance to a single agent. Even though some LLMs today can accept very long input contexts (for instance, Gemini 1.5 Pro accepts 1 million tokens), their ability to truly understand long, complex inputs is mixed. An agentic workflow in which the LLM is prompted to focus on one thing at a time can give better performance. By telling it when it should play software engineer, we can also specify what is important in that role’s subtask. For example, the prompt above emphasized clear, efficient code as opposed to, say, \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "Agentic Design Patterns Part 2: Reflection✨ New course! Enroll in Building AI Voice Agents for ProductionExplore CoursesAI NewsletterThe BatchAndrew's LetterData PointsML ResearchBlogCommunityForumEventsAmbassadorsAmbassador SpotlightResourcesCompanyAboutCareersContactStart LearningWeekly IssuesAndrew's LettersData PointsML ResearchBusinessScienceCultureHardwareAI CareersAboutSubscribeThe BatchLettersArticleAgentic Design Patterns Part 2, Reflection Large language models can become more effective agents by reflecting on their own behavior.LettersTechnical InsightsPublishedMar 27, 2024Reading time2 min readShareDear friends,Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress this year: Reflection, Tool Use, Planning and Multi-agent collaboration. Instead of having an LLM generate its final output directly, an agentic workflow prompts the LLM multiple times, giving it opportunities to build step by step to higher-quality output. In this letter, I'd like to discuss Reflection. For a design pattern that’s relatively quick to implement, I've seen it lead to surprising performance gains. You may have had the experience of prompting ChatGPT/Claude/Gemini, receiving unsatisfactory output, delivering critical feedback to help the LLM improve its response, and then getting a better response. What if you automate the step of delivering critical feedback, so the model automatically criticizes its own output and improves its response? This is the crux of Reflection. Take the task of asking an LLM to write code. We can prompt it to generate the desired code directly to carry out some task X. After that, we can prompt it to reflect on its own output, perhaps as follows:Here’s code intended for task X: [previously generated code]    Check the code carefully for correctness, style, and efficiency, and give constructive criticism for how to improve it.Sometimes this causes the LLM to spot problems and come up with constructive suggestions. Next, we can prompt the LLM with context including (i) the previously generated code and the constructive feedback and (ii) ask it to use the feedback to rewrite the code. This can lead to a better response. Repeating the criticism/rewrite process might yield further improvements. This self-reflection process allows the LLM to spot gaps and improve its output on a variety of tasks including producing \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs_to_use = []\n",
    "for doc in docs:\n",
    "    print(doc.page_content, '\\n', '-'*50)\n",
    "    res = retrieval_grader.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "    print(res,'\\n')\n",
    "    if res.binary_score == 'yes':\n",
    "        docs_to_use.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea57c503-1242-4ec9-9647-0c5de0f187d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The different kinds of agentic design patterns include:\n",
      "\n",
      "- **Planning**: Involves using a large language model (LLM) to autonomously decide the sequence of steps to accomplish a larger task.\n",
      "- **Reflection**: Allows LLMs to improve their outputs by reflecting on their own behavior and providing self-critique.\n",
      "- **Tool Use**: Enables agents to utilize various tools to enhance their capabilities and task execution.\n",
      "- **Multi-Agent Collaboration**: Involves breaking down complex tasks into subtasks assigned to different agents, each specializing in a specific role.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. \n",
    "Use three-to-five sentences maximum and keep the answer concise. Buller point anwser is appreciated.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(f\"<doc{i+1}>:\\nTitle:{doc.metadata['title']}\\nSource:{doc.metadata['source']}\\nContent:{doc.page_content}\\n</doc{i+1}>\\n\" for i, doc in enumerate(docs))\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\":format_docs(docs_to_use), \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17344a-9f11-4c3c-b26b-9c5c04a6ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "Check for Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e50280f-b8ea-43b4-8573-0ce8f0b084ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in 'generation' answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        ...,\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "    Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n <facts>{documents}</facts> \\n\\n LLM generation: <generation>{generation}</generation>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "\n",
    "response = hallucination_grader.invoke({\"documents\": format_docs(docs_to_use), \"generation\": generation})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b702a-0c87-4698-ba4a-7fa5d900606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Highlight used docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46e1e436-356f-46b8-8f71-cd7c52613d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Data model\n",
    "class HighlightDocuments(BaseModel):\n",
    "    \"\"\"Return the specific part of a document used for answering the question.\"\"\"\n",
    "\n",
    "    id: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of id of docs used to answers the question\"\n",
    "    )\n",
    "\n",
    "    title: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of titles used to answers the question\"\n",
    "    )\n",
    "\n",
    "    source: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of sources used to answers the question\"\n",
    "    )\n",
    "\n",
    "    segment: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of direct segements from used documents that answers the question\"\n",
    "    )\n",
    "\n",
    "# parser\n",
    "parser = PydanticOutputParser(pydantic_object=HighlightDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an advanced assistant for document search and retrieval. You are provided with the following:\n",
    "1. A question.\n",
    "2. A generated answer based on the question.\n",
    "3. A set of documents that were referenced in generating the answer.\n",
    "\n",
    "Your task is to identify and extract the exact inline segments from the provided documents that directly correspond to the content used to \n",
    "generate the given answer. The extracted segments must be verbatim snippets from the documents, ensuring a word-for-word match with the text \n",
    "in the provided documents.\n",
    "\n",
    "Ensure that:\n",
    "- (Important) Each segment is an exact match to a part of the document and is fully contained within the document text.\n",
    "- The relevance of each segment to the generated answer is clear and directly supports the answer provided.\n",
    "- (Important) If you didn't used the specific document don't mention it.\n",
    "\n",
    "Used documents: <docs>{documents}</docs> \\n\\n User question: <question>{question}</question> \\n\\n Generated answer: <answer>{generation}</answer>\n",
    "\n",
    "<format_instruction>\n",
    "{format_instructions}\n",
    "</format_instruction>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template= system,\n",
    "    input_variables=[\"documents\", \"question\", \"generation\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "# Chain\n",
    "doc_lookup = prompt | llm | parser\n",
    "\n",
    "# Run\n",
    "lookup_response = doc_lookup.invoke({\"documents\":format_docs(docs_to_use), \"question\": question, \"generation\": generation})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "738598d2-4df1-4c6b-94e1-649e3900cd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: doc2\n",
      "Title: Agentic Design Patterns Part 4: Planning\n",
      "Source: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-4-planning/?ref=dl-staging-website.ghost.io\n",
      "Text Segment: Planning is a key agentic AI design pattern in which we use a large language model (LLM) to autonomously decide on what sequence of steps to execute to accomplish a larger task.\n",
      "\n",
      "ID: doc4\n",
      "Title: Agentic Design Patterns Part 2: Reflection\n",
      "Source: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?ref=dl-staging-website.ghost.io\n",
      "Text Segment: Large language models can become more effective agents by reflecting on their own behavior.\n",
      "\n",
      "ID: doc3\n",
      "Title: Agentic Design Patterns Part 5, Multi-Agent Collaboration\n",
      "Source: https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-5-multi-agent-collaboration/?ref=dl-staging-website.ghost.io\n",
      "Text Segment: Multi-agent collaboration is the last of the four key AI agentic design patterns that I’ve described in recent letters.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for id, title, source, segment in zip(lookup_response.id, lookup_response.title, lookup_response.source, lookup_response.segment):\n",
    "    print(f\"ID: {id}\\nTitle: {title}\\nSource: {source}\\nText Segment: {segment}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5043889-aa15-405f-b469-93f4a66547ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
